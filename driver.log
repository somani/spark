[main] INFO  org.apache.spark.util.SignalUtils  - Registering signal handler for INT
[main] INFO  org.apache.spark.SparkContext  - Running Spark version 3.3.0-SNAPSHOT
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.SparkContext  - Submitted application: Spark shell
[main] INFO  org.apache.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[main] INFO  org.apache.spark.resource.ResourceProfile  - Limiting resource is cpu
[main] INFO  org.apache.spark.resource.ResourceProfileManager  - Added ResourceProfile id: 0
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abhishek.somani); groups with view permissions: Set(); users  with modify permissions: Set(abhishek.somani); groups with modify permissions: Set()
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 62626.
[main] INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
[main] INFO  org.apache.spark.storage.DiskBlockManager  - Created local directory at /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/blockmgr-93d2c746-ecb8-446f-8037-9fb2271e5936
[main] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 408.9 MiB
[main] INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
[main] WARN  org.apache.spark.util.Utils  - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4041.
[main] INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 192.168.0.194
[main] INFO  org.apache.spark.executor.Executor  - Using REPL class URI: spark://192.168.0.194:62626/classes
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62627.
[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  - Server created on 192.168.0.194:62627
[main] INFO  org.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 62627, None)
[dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Registering block manager 192.168.0.194:62627 with 408.9 MiB RAM, BlockManagerId(driver, 192.168.0.194, 62627, None)
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 62627, None)
[main] INFO  org.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 192.168.0.194, 62627, None)
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Invoking stop() from shutdown hook
[shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI  - Stopped Spark web UI at http://192.168.0.194:4041
[dispatcher-event-loop-3] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  - MapOutputTrackerMasterEndpoint stopped!
[shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore cleared
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager  - BlockManager stopped
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster  - BlockManagerMaster stopped
[dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  - OutputCommitCoordinator stopped!
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Successfully stopped SparkContext
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Shutdown hook called
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-046a41ed-33a7-4c0b-ace7-b0657c1d8664
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-d3cf315e-5d3a-4963-bde7-85d83899f90d
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-d3cf315e-5d3a-4963-bde7-85d83899f90d/repl-6aa4193a-b64e-44c4-8e3a-2a86a44eb0a1
[main] INFO  org.apache.spark.util.SignalUtils  - Registering signal handler for INT
[main] INFO  org.apache.spark.SparkContext  - Running Spark version 3.3.0-SNAPSHOT
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.SparkContext  - Submitted application: Spark shell
[main] INFO  org.apache.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[main] INFO  org.apache.spark.resource.ResourceProfile  - Limiting resource is cpu
[main] INFO  org.apache.spark.resource.ResourceProfileManager  - Added ResourceProfile id: 0
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abhishek.somani); groups with view permissions: Set(); users  with modify permissions: Set(abhishek.somani); groups with modify permissions: Set()
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 62642.
[main] INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
[main] INFO  org.apache.spark.storage.DiskBlockManager  - Created local directory at /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/blockmgr-3eb3ac69-df86-482d-aa6a-f1f584de706d
[main] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 408.9 MiB
[main] INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
[main] WARN  org.apache.spark.util.Utils  - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4041.
[main] INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 192.168.0.194
[main] INFO  org.apache.spark.executor.Executor  - Using REPL class URI: spark://192.168.0.194:62642/classes
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62643.
[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  - Server created on 192.168.0.194:62643
[main] INFO  org.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 62643, None)
[dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Registering block manager 192.168.0.194:62643 with 408.9 MiB RAM, BlockManagerId(driver, 192.168.0.194, 62643, None)
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 62643, None)
[main] INFO  org.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 192.168.0.194, 62643, None)
[main] INFO  org.apache.spark.sql.internal.SharedState  - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[main] INFO  org.apache.spark.sql.internal.SharedState  - Warehouse path is 'file:/Users/abhishek.somani/src/spark/spark-warehouse'.
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 147.968459 ms
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 13.517746 ms
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Invoking stop() from shutdown hook
[shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI  - Stopped Spark web UI at http://192.168.0.194:4041
[dispatcher-event-loop-11] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  - MapOutputTrackerMasterEndpoint stopped!
[shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore cleared
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager  - BlockManager stopped
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster  - BlockManagerMaster stopped
[dispatcher-event-loop-15] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  - OutputCommitCoordinator stopped!
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Successfully stopped SparkContext
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Shutdown hook called
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-ef473175-a36a-4a71-ab58-44ef832ad12d
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-ef473175-a36a-4a71-ab58-44ef832ad12d/repl-53daefda-bb73-4914-8148-7bd196ea9e22
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-6afbb05f-08ee-4b2d-83b6-213fe88d516e
[main] INFO  org.apache.spark.util.SignalUtils  - Registering signal handler for INT
[main] INFO  org.apache.spark.SparkContext  - Running Spark version 3.3.0-SNAPSHOT
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.SparkContext  - Submitted application: Spark shell
[main] INFO  org.apache.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[main] INFO  org.apache.spark.resource.ResourceProfile  - Limiting resource is cpu
[main] INFO  org.apache.spark.resource.ResourceProfileManager  - Added ResourceProfile id: 0
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abhishek.somani); groups with view permissions: Set(); users  with modify permissions: Set(abhishek.somani); groups with modify permissions: Set()
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 62649.
[main] INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
[main] INFO  org.apache.spark.storage.DiskBlockManager  - Created local directory at /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/blockmgr-9c5b132c-6658-4867-926f-d721120ed662
[main] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 408.9 MiB
[main] INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
[main] WARN  org.apache.spark.util.Utils  - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4041.
[main] INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 192.168.0.194
[main] INFO  org.apache.spark.executor.Executor  - Using REPL class URI: spark://192.168.0.194:62649/classes
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62650.
[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  - Server created on 192.168.0.194:62650
[main] INFO  org.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 62650, None)
[dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Registering block manager 192.168.0.194:62650 with 408.9 MiB RAM, BlockManagerId(driver, 192.168.0.194, 62650, None)
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 62650, None)
[main] INFO  org.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 192.168.0.194, 62650, None)
[main] INFO  org.apache.spark.sql.internal.SharedState  - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[main] INFO  org.apache.spark.sql.internal.SharedState  - Warehouse path is 'file:/Users/abhishek.somani/src/spark/spark-warehouse'.
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 157.44455 ms
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 13.465931 ms
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Invoking stop() from shutdown hook
[shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI  - Stopped Spark web UI at http://192.168.0.194:4041
[dispatcher-event-loop-14] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  - MapOutputTrackerMasterEndpoint stopped!
[shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore cleared
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager  - BlockManager stopped
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster  - BlockManagerMaster stopped
[dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  - OutputCommitCoordinator stopped!
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Successfully stopped SparkContext
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Shutdown hook called
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-f8cf6af0-f00c-4400-a9e2-96770ed112b6/repl-506f52ed-838c-4c5c-a0c7-098480af4079
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-5e45ae54-4ed4-4fa5-a9b1-29cf79e8202d
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-f8cf6af0-f00c-4400-a9e2-96770ed112b6
[main] INFO  org.apache.spark.util.SignalUtils  - Registering signal handler for INT
[main] INFO  org.apache.spark.SparkContext  - Running Spark version 3.3.0-SNAPSHOT
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.SparkContext  - Submitted application: Spark shell
[main] INFO  org.apache.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[main] INFO  org.apache.spark.resource.ResourceProfile  - Limiting resource is cpu
[main] INFO  org.apache.spark.resource.ResourceProfileManager  - Added ResourceProfile id: 0
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abhishek.somani); groups with view permissions: Set(); users  with modify permissions: Set(abhishek.somani); groups with modify permissions: Set()
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 62738.
[main] INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
[main] INFO  org.apache.spark.storage.DiskBlockManager  - Created local directory at /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/blockmgr-16bfa204-988a-4bff-b75b-d6008f3eb9dd
[main] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 408.9 MiB
[main] INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4040.
[main] INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 192.168.0.194
[main] INFO  org.apache.spark.executor.Executor  - Using REPL class URI: spark://192.168.0.194:62738/classes
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62739.
[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  - Server created on 192.168.0.194:62739
[main] INFO  org.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 62739, None)
[dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Registering block manager 192.168.0.194:62739 with 408.9 MiB RAM, BlockManagerId(driver, 192.168.0.194, 62739, None)
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 62739, None)
[main] INFO  org.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 192.168.0.194, 62739, None)
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Invoking stop() from shutdown hook
[shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI  - Stopped Spark web UI at http://192.168.0.194:4040
[dispatcher-event-loop-12] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  - MapOutputTrackerMasterEndpoint stopped!
[shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore cleared
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager  - BlockManager stopped
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster  - BlockManagerMaster stopped
[dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  - OutputCommitCoordinator stopped!
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Successfully stopped SparkContext
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Shutdown hook called
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-37f9ef5a-d8c0-43ff-8274-42f29522492e/repl-78f88595-f155-4560-82ef-d9625b6c8bf6
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-19bd5ecf-1dc8-467a-b74e-6c24e1756dc5
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-37f9ef5a-d8c0-43ff-8274-42f29522492e
[main] INFO  org.apache.spark.util.SignalUtils  - Registering signal handler for INT
[main] INFO  org.apache.spark.SparkContext  - Running Spark version 3.3.0-SNAPSHOT
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.SparkContext  - Submitted application: Spark shell
[main] INFO  org.apache.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[main] INFO  org.apache.spark.resource.ResourceProfile  - Limiting resource is cpu
[main] INFO  org.apache.spark.resource.ResourceProfileManager  - Added ResourceProfile id: 0
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abhishek.somani); groups with view permissions: Set(); users  with modify permissions: Set(abhishek.somani); groups with modify permissions: Set()
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 62743.
[main] INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
[main] INFO  org.apache.spark.storage.DiskBlockManager  - Created local directory at /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/blockmgr-16ba956c-3f4d-4faa-997a-6b2ca1c9b649
[main] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 408.9 MiB
[main] INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4040.
[main] INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 192.168.0.194
[main] INFO  org.apache.spark.executor.Executor  - Using REPL class URI: spark://192.168.0.194:62743/classes
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62744.
[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  - Server created on 192.168.0.194:62744
[main] INFO  org.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 62744, None)
[dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Registering block manager 192.168.0.194:62744 with 408.9 MiB RAM, BlockManagerId(driver, 192.168.0.194, 62744, None)
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 62744, None)
[main] INFO  org.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 192.168.0.194, 62744, None)
[main] INFO  org.apache.spark.sql.internal.SharedState  - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[main] INFO  org.apache.spark.sql.internal.SharedState  - Warehouse path is 'file:/Users/abhishek.somani/src/spark/spark-warehouse'.
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 169.990218 ms
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 14.532537 ms
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Invoking stop() from shutdown hook
[shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI  - Stopped Spark web UI at http://192.168.0.194:4040
[dispatcher-event-loop-15] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  - MapOutputTrackerMasterEndpoint stopped!
[shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore cleared
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager  - BlockManager stopped
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster  - BlockManagerMaster stopped
[dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  - OutputCommitCoordinator stopped!
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Successfully stopped SparkContext
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Shutdown hook called
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-2b1a83d6-ba80-4b64-9aac-976f35e88487
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-413302bd-7f39-4b66-9bd1-2908b0a5788f
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-2b1a83d6-ba80-4b64-9aac-976f35e88487/repl-a70139cd-87cb-4807-85fe-94e2f839abd0
[main] INFO  org.apache.spark.util.SignalUtils  - Registering signal handler for INT
[main] INFO  org.apache.spark.SparkContext  - Running Spark version 3.3.0-SNAPSHOT
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.SparkContext  - Submitted application: Spark shell
[main] INFO  org.apache.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[main] INFO  org.apache.spark.resource.ResourceProfile  - Limiting resource is cpu
[main] INFO  org.apache.spark.resource.ResourceProfileManager  - Added ResourceProfile id: 0
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abhishek.somani); groups with view permissions: Set(); users  with modify permissions: Set(abhishek.somani); groups with modify permissions: Set()
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 62761.
[main] INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
[main] INFO  org.apache.spark.storage.DiskBlockManager  - Created local directory at /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/blockmgr-ca941479-9b39-4694-9986-f1859f85e94a
[main] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 408.9 MiB
[main] INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4040.
[main] INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 192.168.0.194
[main] INFO  org.apache.spark.executor.Executor  - Using REPL class URI: spark://192.168.0.194:62761/classes
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62762.
[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  - Server created on 192.168.0.194:62762
[main] INFO  org.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 62762, None)
[dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Registering block manager 192.168.0.194:62762 with 408.9 MiB RAM, BlockManagerId(driver, 192.168.0.194, 62762, None)
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 62762, None)
[main] INFO  org.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 192.168.0.194, 62762, None)
[main] INFO  org.apache.spark.sql.internal.SharedState  - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[main] INFO  org.apache.spark.sql.internal.SharedState  - Warehouse path is 'file:/Users/abhishek.somani/src/spark/spark-warehouse'.
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 180.095199 ms
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 17.083871 ms
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Invoking stop() from shutdown hook
[shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI  - Stopped Spark web UI at http://192.168.0.194:4040
[dispatcher-event-loop-2] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  - MapOutputTrackerMasterEndpoint stopped!
[shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore cleared
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager  - BlockManager stopped
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster  - BlockManagerMaster stopped
[dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  - OutputCommitCoordinator stopped!
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Successfully stopped SparkContext
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Shutdown hook called
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-081626dc-5d63-4cb7-8824-8ffaf9e31dde
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-e3fedbfd-43e6-41f3-9177-36e9c0da9e88/repl-0541a53f-0cbd-40a2-b43f-d1a3627998a9
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-e3fedbfd-43e6-41f3-9177-36e9c0da9e88
[main] INFO  org.apache.spark.util.SignalUtils  - Registering signal handler for INT
[main] INFO  org.apache.hadoop.hive.conf.HiveConf  - Found configuration file file:/Users/abhishek.somani/src/spark/conf/hive-site.xml
[main] INFO  org.apache.spark.SparkContext  - Running Spark version 3.3.0-SNAPSHOT
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.SparkContext  - Submitted application: Spark shell
[main] INFO  org.apache.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[main] INFO  org.apache.spark.resource.ResourceProfile  - Limiting resource is cpu
[main] INFO  org.apache.spark.resource.ResourceProfileManager  - Added ResourceProfile id: 0
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abhishek.somani); groups with view permissions: Set(); users  with modify permissions: Set(abhishek.somani); groups with modify permissions: Set()
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 62906.
[main] INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
[main] INFO  org.apache.spark.storage.DiskBlockManager  - Created local directory at /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/blockmgr-20d00a74-40a0-4528-9931-ab3488239458
[main] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 408.9 MiB
[main] INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4040.
[main] INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 192.168.0.194
[main] INFO  org.apache.spark.executor.Executor  - Using REPL class URI: spark://192.168.0.194:62906/classes
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62907.
[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  - Server created on 192.168.0.194:62907
[main] INFO  org.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 62907, None)
[dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Registering block manager 192.168.0.194:62907 with 408.9 MiB RAM, BlockManagerId(driver, 192.168.0.194, 62907, None)
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 62907, None)
[main] INFO  org.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 192.168.0.194, 62907, None)
[main] INFO  org.apache.spark.sql.internal.SharedState  - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[main] INFO  org.apache.spark.sql.internal.SharedState  - Warehouse path is 'file:/Users/abhishek.somani/src/spark/spark-warehouse'.
[main] INFO  org.apache.spark.sql.hive.HiveUtils  - Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
[main] INFO  org.apache.hadoop.hive.conf.HiveConf  - Found configuration file file:/Users/abhishek.somani/src/spark/conf/hive-site.xml
[main] INFO  org.apache.spark.sql.hive.client.HiveClientImpl  - Warehouse location for Hive client (version 2.3.9) is file:/Users/abhishek.somani/src/spark/spark-warehouse
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.jdbc.timeout does not exist
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.retries.wait does not exist
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - ObjectStore, initialize called
[main] INFO  DataNucleus.Persistence  - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property datanucleus.cache.level2 unknown - will be ignored
[main] ERROR DataNucleus.Datastore  - Exception thrown creating StoreManager. See the nested exception
Error creating transactional connection factory
org.datanucleus.exceptions.NucleusException: Error creating transactional connection factory
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:214)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	... 157 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
Nested Throwables StackTrace:
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
[main] WARN  org.apache.hadoop.hive.metastore.HiveMetaStore  - Retrying creating default database after error: Error creating transactional connection factory
javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:671)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:830)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
NestedThrowablesStackTrace:
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.jdbc.timeout does not exist
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.retries.wait does not exist
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - ObjectStore, initialize called
[main] INFO  DataNucleus.Persistence  - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property datanucleus.cache.level2 unknown - will be ignored
[main] ERROR DataNucleus.Datastore  - Exception thrown creating StoreManager. See the nested exception
Error creating transactional connection factory
org.datanucleus.exceptions.NucleusException: Error creating transactional connection factory
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:214)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	... 157 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
Nested Throwables StackTrace:
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.jdbc.timeout does not exist
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.retries.wait does not exist
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - ObjectStore, initialize called
[main] INFO  DataNucleus.Persistence  - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property datanucleus.cache.level2 unknown - will be ignored
[main] ERROR DataNucleus.Datastore  - Exception thrown creating StoreManager. See the nested exception
Error creating transactional connection factory
org.datanucleus.exceptions.NucleusException: Error creating transactional connection factory
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:214)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	... 157 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
Nested Throwables StackTrace:
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
[main] WARN  org.apache.hadoop.hive.metastore.HiveMetaStore  - Retrying creating default database after error: Error creating transactional connection factory
javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:671)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:830)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
NestedThrowablesStackTrace:
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.jdbc.timeout does not exist
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.retries.wait does not exist
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - ObjectStore, initialize called
[main] INFO  DataNucleus.Persistence  - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property datanucleus.cache.level2 unknown - will be ignored
[main] ERROR DataNucleus.Datastore  - Exception thrown creating StoreManager. See the nested exception
Error creating transactional connection factory
org.datanucleus.exceptions.NucleusException: Error creating transactional connection factory
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:214)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	... 157 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
Nested Throwables StackTrace:
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.jdbc.timeout does not exist
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.retries.wait does not exist
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - ObjectStore, initialize called
[main] INFO  DataNucleus.Persistence  - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property datanucleus.cache.level2 unknown - will be ignored
[main] ERROR DataNucleus.Datastore  - Exception thrown creating StoreManager. See the nested exception
Error creating transactional connection factory
org.datanucleus.exceptions.NucleusException: Error creating transactional connection factory
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:214)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	... 157 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
Nested Throwables StackTrace:
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
[main] WARN  org.apache.hadoop.hive.metastore.HiveMetaStore  - Retrying creating default database after error: Error creating transactional connection factory
javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:671)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:830)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
NestedThrowablesStackTrace:
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Invoking stop() from shutdown hook
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.jdbc.timeout does not exist
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.retries.wait does not exist
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - ObjectStore, initialize called
[main] INFO  DataNucleus.Persistence  - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property datanucleus.cache.level2 unknown - will be ignored
[main] ERROR DataNucleus.Datastore  - Exception thrown creating StoreManager. See the nested exception
Error creating transactional connection factory
org.datanucleus.exceptions.NucleusException: Error creating transactional connection factory
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:214)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	... 157 more
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
Nested Throwables StackTrace:
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)
	at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)
	at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1975)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1970)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1177)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:814)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:659)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:232)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
	... 164 more
Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.
	at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)
	at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)
	... 166 more
[shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI  - Stopped Spark web UI at http://192.168.0.194:4040
[dispatcher-event-loop-14] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  - MapOutputTrackerMasterEndpoint stopped!
[shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore cleared
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager  - BlockManager stopped
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster  - BlockManagerMaster stopped
[dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  - OutputCommitCoordinator stopped!
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Successfully stopped SparkContext
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Shutdown hook called
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-22cd7cec-6a7b-4441-b67a-f73ccd27a5b5
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-22cd7cec-6a7b-4441-b67a-f73ccd27a5b5/repl-77006d17-ea0c-4904-b8b7-3033b227355f
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-d5af88e8-744f-44a6-88e9-edf08a1ce33a
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] INFO  org.apache.spark.util.SignalUtils  - Registering signal handler for INT
[main] INFO  org.apache.hadoop.hive.conf.HiveConf  - Found configuration file file:/Users/abhishek.somani/src/spark/conf/hive-site.xml
[main] INFO  org.apache.spark.SparkContext  - Running Spark version 3.3.0-SNAPSHOT
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.SparkContext  - Submitted application: Spark shell
[main] INFO  org.apache.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[main] INFO  org.apache.spark.resource.ResourceProfile  - Limiting resource is cpu
[main] INFO  org.apache.spark.resource.ResourceProfileManager  - Added ResourceProfile id: 0
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abhishek.somani); groups with view permissions: Set(); users  with modify permissions: Set(abhishek.somani); groups with modify permissions: Set()
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 62910.
[main] INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
[main] INFO  org.apache.spark.storage.DiskBlockManager  - Created local directory at /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/blockmgr-f45218a3-a71b-48ff-b6c2-adda3445d3e4
[main] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 408.9 MiB
[main] INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4040.
[main] INFO  org.apache.spark.SparkContext  - Added JAR file:///Users/abhishek.somani/Downloads/mysql-connector-java-5.1.26.jar at spark://192.168.0.194:62910/jars/mysql-connector-java-5.1.26.jar with timestamp 1634574149182
[main] INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 192.168.0.194
[main] INFO  org.apache.spark.executor.Executor  - Using REPL class URI: spark://192.168.0.194:62910/classes
[main] INFO  org.apache.spark.executor.Executor  - Fetching spark://192.168.0.194:62910/jars/mysql-connector-java-5.1.26.jar with timestamp 1634574149182
[main] INFO  org.apache.spark.network.client.TransportClientFactory  - Successfully created connection to /192.168.0.194:62910 after 35 ms (0 ms spent in bootstraps)
[main] INFO  org.apache.spark.util.Utils  - Fetching spark://192.168.0.194:62910/jars/mysql-connector-java-5.1.26.jar to /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-4bed00a7-0cb7-447c-8e18-10668fea644f/userFiles-45ab7269-e613-4492-b8b2-4f8979e40d10/fetchFileTemp7495656201330380299.tmp
[main] INFO  org.apache.spark.executor.Executor  - Adding file:/private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-4bed00a7-0cb7-447c-8e18-10668fea644f/userFiles-45ab7269-e613-4492-b8b2-4f8979e40d10/mysql-connector-java-5.1.26.jar to class loader
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62912.
[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  - Server created on 192.168.0.194:62912
[main] INFO  org.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 62912, None)
[dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Registering block manager 192.168.0.194:62912 with 408.9 MiB RAM, BlockManagerId(driver, 192.168.0.194, 62912, None)
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 62912, None)
[main] INFO  org.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 192.168.0.194, 62912, None)
[main] INFO  org.apache.spark.sql.internal.SharedState  - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[main] INFO  org.apache.spark.sql.internal.SharedState  - Warehouse path is 'file:/Users/abhishek.somani/src/spark/spark-warehouse'.
[main] INFO  org.apache.spark.sql.hive.HiveUtils  - Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
[main] INFO  org.apache.hadoop.hive.conf.HiveConf  - Found configuration file file:/Users/abhishek.somani/src/spark/conf/hive-site.xml
[main] INFO  org.apache.spark.sql.hive.client.HiveClientImpl  - Warehouse location for Hive client (version 2.3.9) is file:/Users/abhishek.somani/src/spark/spark-warehouse
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.jdbc.timeout does not exist
[main] WARN  org.apache.hadoop.hive.conf.HiveConf  - HiveConf of name hive.stats.retries.wait does not exist
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - ObjectStore, initialize called
[main] INFO  DataNucleus.Persistence  - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property datanucleus.cache.level2 unknown - will be ignored
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
[main] WARN  DataNucleus.Query  - Query for candidates of org.apache.hadoop.hive.metastore.model.MConstraint and subclasses resulted in no possible candidates
Required table missing : "`KEY_CONSTRAINTS`" in Catalog "" Schema "". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable "datanucleus.schema.autoCreateTables"
org.datanucleus.store.rdbms.exceptions.MissingTableException: Required table missing : "`KEY_CONSTRAINTS`" in Catalog "" Schema "". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable "datanucleus.schema.autoCreateTables"
	at org.datanucleus.store.rdbms.table.AbstractTable.exists(AbstractTable.java:606)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3385)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2896)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:119)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.manageClasses(RDBMSStoreManager.java:1627)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:672)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:425)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:865)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:347)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1816)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1744)
	at org.datanucleus.store.query.Query.execute(Query.java:1726)
	at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:374)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:216)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:190)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:144)
	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:410)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:628)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:594)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:60)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:118)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
	at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:23)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:27)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:29)
	at $line14.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:31)
	at $line14.$read$$iw$$iw$$iw$$iw.<init>(<console>:33)
	at $line14.$read$$iw$$iw$$iw.<init>(<console>:35)
	at $line14.$read$$iw$$iw.<init>(<console>:37)
	at $line14.$read$$iw.<init>(<console>:39)
	at $line14.$read.<init>(<console>:41)
	at $line14.$read$.<init>(<console>:45)
	at $line14.$read$.<clinit>(<console>)
	at $line14.$eval$.$print$lzycompute(<console>:7)
	at $line14.$eval$.$print(<console>:6)
	at $line14.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)
	at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:865)
	at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:733)
	at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:435)
	at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:456)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[main] INFO  org.apache.hadoop.hive.metastore.MetaStoreDirectSql  - Using direct SQL, underlying DB is MYSQL
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - Initialized ObjectStore
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Invoking stop() from shutdown hook
[shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI  - Stopped Spark web UI at http://192.168.0.194:4040
[dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  - MapOutputTrackerMasterEndpoint stopped!
[shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore cleared
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager  - BlockManager stopped
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster  - BlockManagerMaster stopped
[dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  - OutputCommitCoordinator stopped!
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Successfully stopped SparkContext
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Shutdown hook called
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-4bed00a7-0cb7-447c-8e18-10668fea644f/repl-c0408d2b-fe20-4379-af72-0f49151444b6
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-40ce5704-02b5-4e46-89a0-a8f400cb18b5
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-4bed00a7-0cb7-447c-8e18-10668fea644f
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] INFO  org.apache.spark.util.SignalUtils  - Registering signal handler for INT
[main] INFO  org.apache.hadoop.hive.conf.HiveConf  - Found configuration file file:/Users/abhishek.somani/src/spark/conf/hive-site.xml
[main] INFO  org.apache.spark.SparkContext  - Running Spark version 3.3.0-SNAPSHOT
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.SparkContext  - Submitted application: Spark shell
[main] INFO  org.apache.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[main] INFO  org.apache.spark.resource.ResourceProfile  - Limiting resource is cpu
[main] INFO  org.apache.spark.resource.ResourceProfileManager  - Added ResourceProfile id: 0
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abhishek.somani); groups with view permissions: Set(); users  with modify permissions: Set(abhishek.somani); groups with modify permissions: Set()
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 62923.
[main] INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
[main] INFO  org.apache.spark.storage.DiskBlockManager  - Created local directory at /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/blockmgr-eadac3d0-6302-4829-ba63-be377df9a126
[main] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 408.9 MiB
[main] INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4040.
[main] INFO  org.apache.spark.SparkContext  - Added JAR file:///Users/abhishek.somani/Downloads/mysql-connector-java-5.1.26.jar at spark://192.168.0.194:62923/jars/mysql-connector-java-5.1.26.jar with timestamp 1634574179333
[main] INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 192.168.0.194
[main] INFO  org.apache.spark.executor.Executor  - Using REPL class URI: spark://192.168.0.194:62923/classes
[main] INFO  org.apache.spark.executor.Executor  - Fetching spark://192.168.0.194:62923/jars/mysql-connector-java-5.1.26.jar with timestamp 1634574179333
[main] INFO  org.apache.spark.network.client.TransportClientFactory  - Successfully created connection to /192.168.0.194:62923 after 31 ms (0 ms spent in bootstraps)
[main] INFO  org.apache.spark.util.Utils  - Fetching spark://192.168.0.194:62923/jars/mysql-connector-java-5.1.26.jar to /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-cbdf08be-fc10-4cd6-ace0-e21ae2049d18/userFiles-093332ae-3ab4-4317-99a8-5ce7d1909882/fetchFileTemp1756559350691869925.tmp
[main] INFO  org.apache.spark.executor.Executor  - Adding file:/private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-cbdf08be-fc10-4cd6-ace0-e21ae2049d18/userFiles-093332ae-3ab4-4317-99a8-5ce7d1909882/mysql-connector-java-5.1.26.jar to class loader
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62925.
[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  - Server created on 192.168.0.194:62925
[main] INFO  org.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 62925, None)
[dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Registering block manager 192.168.0.194:62925 with 408.9 MiB RAM, BlockManagerId(driver, 192.168.0.194, 62925, None)
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 62925, None)
[main] INFO  org.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 192.168.0.194, 62925, None)
[main] INFO  org.apache.spark.sql.internal.SharedState  - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[main] INFO  org.apache.spark.sql.internal.SharedState  - Warehouse path is 'file:/Users/abhishek.somani/src/spark/spark-warehouse'.
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Invoking stop() from shutdown hook
[shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI  - Stopped Spark web UI at http://192.168.0.194:4040
[dispatcher-event-loop-3] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  - MapOutputTrackerMasterEndpoint stopped!
[shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore cleared
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager  - BlockManager stopped
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster  - BlockManagerMaster stopped
[dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  - OutputCommitCoordinator stopped!
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Successfully stopped SparkContext
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Shutdown hook called
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-cbdf08be-fc10-4cd6-ace0-e21ae2049d18
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-cbdf08be-fc10-4cd6-ace0-e21ae2049d18/repl-aaad32b6-02cf-4ab6-afa3-abe19718c220
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-a2c49655-541a-4677-ba0e-8e383b72df55
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] INFO  org.apache.spark.util.SignalUtils  - Registering signal handler for INT
[main] INFO  org.apache.hadoop.hive.conf.HiveConf  - Found configuration file file:/Users/abhishek.somani/src/spark/conf/hive-site.xml
[main] INFO  org.apache.spark.SparkContext  - Running Spark version 3.3.0-SNAPSHOT
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.SparkContext  - Submitted application: Spark shell
[main] INFO  org.apache.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[main] INFO  org.apache.spark.resource.ResourceProfile  - Limiting resource is cpu
[main] INFO  org.apache.spark.resource.ResourceProfileManager  - Added ResourceProfile id: 0
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abhishek.somani); groups with view permissions: Set(); users  with modify permissions: Set(abhishek.somani); groups with modify permissions: Set()
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 62926.
[main] INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
[main] INFO  org.apache.spark.storage.DiskBlockManager  - Created local directory at /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/blockmgr-01390830-222b-42e6-9bca-9aef9110e4ac
[main] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 408.9 MiB
[main] INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4040.
[main] INFO  org.apache.spark.SparkContext  - Added JAR file:///Users/abhishek.somani/Downloads/mysql-connector-java-5.1.26.jar at spark://192.168.0.194:62926/jars/mysql-connector-java-5.1.26.jar with timestamp 1634574210121
[main] INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 192.168.0.194
[main] INFO  org.apache.spark.executor.Executor  - Using REPL class URI: spark://192.168.0.194:62926/classes
[main] INFO  org.apache.spark.executor.Executor  - Fetching spark://192.168.0.194:62926/jars/mysql-connector-java-5.1.26.jar with timestamp 1634574210121
[main] INFO  org.apache.spark.network.client.TransportClientFactory  - Successfully created connection to /192.168.0.194:62926 after 31 ms (0 ms spent in bootstraps)
[main] INFO  org.apache.spark.util.Utils  - Fetching spark://192.168.0.194:62926/jars/mysql-connector-java-5.1.26.jar to /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-2b411dc0-2ed4-4a6e-8d5e-1f63c1008be4/userFiles-6760572d-6e82-4537-bbbe-af1bf5e84d3b/fetchFileTemp6137933796454743726.tmp
[main] INFO  org.apache.spark.executor.Executor  - Adding file:/private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-2b411dc0-2ed4-4a6e-8d5e-1f63c1008be4/userFiles-6760572d-6e82-4537-bbbe-af1bf5e84d3b/mysql-connector-java-5.1.26.jar to class loader
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62929.
[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  - Server created on 192.168.0.194:62929
[main] INFO  org.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 62929, None)
[dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Registering block manager 192.168.0.194:62929 with 408.9 MiB RAM, BlockManagerId(driver, 192.168.0.194, 62929, None)
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 62929, None)
[main] INFO  org.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 192.168.0.194, 62929, None)
[main] INFO  org.apache.spark.sql.internal.SharedState  - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[main] INFO  org.apache.spark.sql.internal.SharedState  - Warehouse path is 'file:/Users/abhishek.somani/src/spark/spark-warehouse'.
[main] INFO  org.apache.spark.sql.hive.HiveUtils  - Initializing HiveMetastoreConnection version 0.13.1 using file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/bonecp-configs.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver-dns_io.netty__netty-resolver-dns__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec_io.netty__netty-codec__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http2_io.netty__netty-codec-http2__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-annotations_com.fasterxml.jackson.core__jackson-annotations__2.11.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-stub_io.grpc__grpc-stub__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.23--org.spark-project.hive.shims__hive-shims-0.23__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-haproxy_io.netty__netty-codec-haproxy__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc-protocol_com.linecorp.armeria__armeria-grpc-protocol__1.0.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--client--client-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-databind_com.fasterxml.jackson.core__jackson-databind__2.11.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_liball_deps_2.12_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-brave_com.linecorp.armeria__armeria-brave__1.0.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/extern--libaws-regions.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.hdrhistogram_HdrHistogram_org.hdrhistogram__HdrHistogram__2.1.12_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.j2objc_j2objc-annotations_com.google.j2objc__j2objc-annotations__1.3_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty-client--jetty-io_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----jackson_annotations_shaded--libjackson-annotations.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter_io.zipkin.reporter2__zipkin-reporter__2.15.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.micrometer_micrometer-core_io.micrometer__micrometer-core__1.5.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--jline--jline--jline__jline__0.9.94.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-tcnative-boringssl-static_io.netty__netty-tcnative-boringssl-static__2.0.31.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.2.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/dbfs--exceptions--exceptions-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.595.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jackson--jsr305_only_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--tracing--tracing-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter-brave_io.zipkin.reporter2__zipkin-reporter-brave__2.15.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_2.4_2.11_deploy_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-common--org.spark-project.hive__hive-common__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common--org.spark-project.hive.shims__hive-shims-common__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_net.bytebuddy_byte-buddy_net.bytebuddy__byte-buddy__1.10.9_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/api-base--api-base-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty-client--jetty-http_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.android_annotations_com.google.android__annotations__4.1.1.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-core_io.grpc__grpc-core__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-ant--org.spark-project.hive__hive-ant__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--log4j--log4j--log4j__log4j__1.2.17.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-context_io.grpc__grpc-context__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/extern--extern-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-dns_io.netty__netty-codec-dns__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--io.netty--netty--io.netty__netty__3.8.0.Final.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-services_io.grpc__grpc-services__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.codehaus.mojo_animal-sniffer-annotations_org.codehaus.mojo__animal-sniffer-annotations__1.18_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--com.microsoft.azure__azure-client-runtime__1.7.8_container_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--antlr--antlr--antlr__antlr__2.7.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_2.4_2.11_deploy_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common_io.netty__netty-transport-native-unix-common__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--path--path-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java_com.google.protobuf__protobuf-java__3.12.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf_io.grpc__grpc-protobuf__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty8-shaded-client--jetty-util_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.10.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.guava_failureaccess_com.google.guava__failureaccess__1.0.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-handler-proxy_io.netty__netty-handler-proxy__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/s3--s3-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.api.grpc_proto-google-common-protos_com.google.api.grpc__proto-google-common-protos__1.17.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.errorprone_error_prone_annotations_com.google.errorprone__error_prone_annotations__2.3.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.latencyutils_LatencyUtils_org.latencyutils__LatencyUtils__2.0.3_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/s3commit--common--common-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.1.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jackson--guava_only_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/api-base--api-base_java-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.checkerframework_checker-compat-qual_org.checkerframework__checker-compat-qual__2.5.5_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--javolution--javolution--javolution__javolution__5.5.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.47.Final.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty-client--jetty-util_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty-client--jetty-client_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-buffer_io.netty__netty-buffer__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jackson--jackson-module-scala-shaded_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.zipkin.zipkin2_zipkin_io.zipkin.zipkin2__zipkin__2.21.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.12.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-service--org.spark-project.hive__hive-service__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--jetty--client--client-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http_io.netty__netty-codec-http__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--com.microsoft.rest__client-runtime__1.7.8_container_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--util--locks-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.reactivestreams_reactive-streams_org.reactivestreams__reactive-streams__1.0.3_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-core_com.fasterxml.jackson.core__jackson-core__2.11.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf-lite_io.grpc__grpc-protobuf-lite__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.595.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--oro--oro--oro__oro__2.0.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.595.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.guava_listenablefuture_com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave_io.zipkin.brave__brave__5.12.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.595.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20--org.spark-project.hive.shims__hive-shims-0.20__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave-instrumentation-http_io.zipkin.brave__brave-instrumentation-http__5.12.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java-util_com.google.protobuf__protobuf-java-util__3.12.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_2.4_2.11_deploy_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--stax--stax-api--stax__stax-api__1.0.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.curioswitch.curiostack_protobuf-jackson_org.curioswitch.curiostack__protobuf-jackson__1.1.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria_com.linecorp.armeria__armeria__1.0.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----jackson_core_shaded--libjackson-core.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.595.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.code.findbugs_jsr305_com.google.code.findbugs__jsr305__3.0.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_jakarta.annotation_jakarta.annotation-api_jakarta.annotation__jakarta.annotation-api__1.3.5_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-handler_io.netty__netty-handler__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.3.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--datalake--datalake-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.protobuf--protobuf-java--org.spark-project.protobuf__protobuf-java__2.5.0-spark.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.joda_joda-convert_org.joda__joda-convert__2.2.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.guava_guava_com.google.guava__guava__29.0-android_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.12.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc_com.linecorp.armeria__armeria-grpc__1.0.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty8-shaded-client--jetty-io_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--reflection--reflection-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--credentials--credentials-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-api_io.grpc__grpc-api__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----scalapb_090--com.lihaoyi__sourcecode_2.11__0.1.6_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.json--json--org.json__json__20090211.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-epoll-linux-x86_64_io.netty__netty-transport-native-epoll-linux-x86_64__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----scalapb_090--com.lihaoyi__fastparse_2.11__2.1.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver_io.netty__netty-resolver__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.perfmark_perfmark-api_io.perfmark__perfmark-api__0.19.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20S--org.spark-project.hive.shims__hive-shims-0.20S__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--lazy--lazy-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--prometheus-client--simpleclient-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--junit--junit--junit__junit__3.8.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-serde--org.spark-project.hive__hive-serde__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----jackson_databind_shaded--libjackson-databind.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/daemon--data--client--client-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-common_io.netty__netty-common__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common-secure--org.spark-project.hive.shims__hive-shims-common-secure__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jackson--paranamer_only_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/s3commit--client--client-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-transport_io.netty__netty-transport__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-socks_io.netty__netty-codec-socks__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.code.gson_gson_com.google.code.gson__gson__2.8.6_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.11_deploy_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common-linux-x86_64_io.netty__netty-transport-native-unix-common-linux-x86_64__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-shims--org.spark-project.hive__hive-shims__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--prometheus-client--simpleclient_dropwizard-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.27.v20190418.jar
[main] INFO  org.apache.spark.sql.hive.client.HiveClientImpl  - Warehouse location for Hive client (version 0.13.1) is file:/Users/abhishek.somani/src/spark/spark-warehouse
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - ObjectStore, initialize called
[main] INFO  DataNucleus.Persistence  - Property spark.hadoop.datanucleus.schema.autoCreateTables unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property datanucleus.schema.autoCreateTables unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property datanucleus.cache.level2 unknown - will be ignored
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Query  - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - Initialized ObjectStore
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - Added admin role in metastore
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - Added public role in metastore
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - No user is added in admin role, since config is empty
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: global_temp
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: global_temp	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: drop_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=drop_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: create_table: Table(tableName:v, dbName:default, owner:abhishek.somani, createTime:1634574235, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:A, type:int, comment:null), FieldSchema(name:B, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{view.query.out.col.0=A, view.query.out.numCols=2, view.catalogAndNamespace.part.0=spark_catalog, view.catalogAndNamespace.numParts=2, view.query.out.col.1=B, view.referredTempViewNames=[], spark.sql.sources.schema={"type":"struct","fields":[{"name":"A","type":"integer","nullable":false,"metadata":{}},{"name":"B","type":"integer","nullable":false,"metadata":{}}]}, view.catalogAndNamespace.part.1=default, view.referredTempFunctionsNames=[], spark.sql.create.version=3.3.0-SNAPSHOT}, viewOriginalText:SELECT 1 AS A, 1 AS B, viewExpandedText:SELECT 1 AS A, 1 AS B, tableType:VIRTUAL_VIEW)
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=create_table: Table(tableName:v, dbName:default, owner:abhishek.somani, createTime:1634574235, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:A, type:int, comment:null), FieldSchema(name:B, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{view.query.out.col.0=A, view.query.out.numCols=2, view.catalogAndNamespace.part.0=spark_catalog, view.catalogAndNamespace.numParts=2, view.query.out.col.1=B, view.referredTempViewNames=[], spark.sql.sources.schema={"type":"struct","fields":[{"name":"A","type":"integer","nullable":false,"metadata":{}},{"name":"B","type":"integer","nullable":false,"metadata":{}}]}, view.catalogAndNamespace.part.1=default, view.referredTempFunctionsNames=[], spark.sql.create.version=3.3.0-SNAPSHOT}, viewOriginalText:SELECT 1 AS A, 1 AS B, viewExpandedText:SELECT 1 AS A, 1 AS B, tableType:VIRTUAL_VIEW)	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 196.998279 ms
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 14.045167 ms
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: alter_table: db=default tbl=v newtbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=alter_table: db=default tbl=v newtbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Invoking stop() from shutdown hook
[shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI  - Stopped Spark web UI at http://192.168.0.194:4040
[dispatcher-event-loop-7] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  - MapOutputTrackerMasterEndpoint stopped!
[shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore cleared
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager  - BlockManager stopped
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster  - BlockManagerMaster stopped
[dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  - OutputCommitCoordinator stopped!
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Successfully stopped SparkContext
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Shutdown hook called
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-34898ffb-6513-4bdb-9e2e-8b0953b07cf3
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-2b411dc0-2ed4-4a6e-8d5e-1f63c1008be4
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-2b411dc0-2ed4-4a6e-8d5e-1f63c1008be4/repl-b67b49f8-738a-4556-bd89-17edbdae6258
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] INFO  org.apache.spark.util.SignalUtils  - Registering signal handler for INT
[main] INFO  org.apache.hadoop.hive.conf.HiveConf  - Found configuration file file:/Users/abhishek.somani/src/spark/conf/hive-site.xml
[main] INFO  org.apache.spark.SparkContext  - Running Spark version 3.3.0-SNAPSHOT
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.resource.ResourceUtils  - No custom resources configured for spark.driver.
[main] INFO  org.apache.spark.resource.ResourceUtils  - ==============================================================
[main] INFO  org.apache.spark.SparkContext  - Submitted application: Spark shell
[main] INFO  org.apache.spark.resource.ResourceProfile  - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[main] INFO  org.apache.spark.resource.ResourceProfile  - Limiting resource is cpu
[main] INFO  org.apache.spark.resource.ResourceProfileManager  - Added ResourceProfile id: 0
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls to: abhishek.somani
[main] INFO  org.apache.spark.SecurityManager  - Changing view acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - Changing modify acls groups to: 
[main] INFO  org.apache.spark.SecurityManager  - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abhishek.somani); groups with view permissions: Set(); users  with modify permissions: Set(abhishek.somani); groups with modify permissions: Set()
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'sparkDriver' on port 63367.
[main] INFO  org.apache.spark.SparkEnv  - Registering MapOutputTracker
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMaster
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[main] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - BlockManagerMasterEndpoint up
[main] INFO  org.apache.spark.SparkEnv  - Registering BlockManagerMasterHeartbeat
[main] INFO  org.apache.spark.storage.DiskBlockManager  - Created local directory at /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/blockmgr-6b9844b2-d642-4a8c-8d27-b9decc7f0d1a
[main] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore started with capacity 408.9 MiB
[main] INFO  org.apache.spark.SparkEnv  - Registering OutputCommitCoordinator
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'SparkUI' on port 4040.
[main] INFO  org.apache.spark.SparkContext  - Added JAR file:///Users/abhishek.somani/Downloads/mysql-connector-java-5.1.26.jar at spark://192.168.0.194:63367/jars/mysql-connector-java-5.1.26.jar with timestamp 1634577585131
[main] INFO  org.apache.spark.executor.Executor  - Starting executor ID driver on host 192.168.0.194
[main] INFO  org.apache.spark.executor.Executor  - Using REPL class URI: spark://192.168.0.194:63367/classes
[main] INFO  org.apache.spark.executor.Executor  - Fetching spark://192.168.0.194:63367/jars/mysql-connector-java-5.1.26.jar with timestamp 1634577585131
[main] INFO  org.apache.spark.network.client.TransportClientFactory  - Successfully created connection to /192.168.0.194:63367 after 38 ms (0 ms spent in bootstraps)
[main] INFO  org.apache.spark.util.Utils  - Fetching spark://192.168.0.194:63367/jars/mysql-connector-java-5.1.26.jar to /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-b7349d3e-46bb-42b4-9747-b86da77daa09/userFiles-deedd3d1-04c1-4ddb-a46a-f97d473a2bf1/fetchFileTemp2391044230135403417.tmp
[main] INFO  org.apache.spark.executor.Executor  - Adding file:/private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-b7349d3e-46bb-42b4-9747-b86da77daa09/userFiles-deedd3d1-04c1-4ddb-a46a-f97d473a2bf1/mysql-connector-java-5.1.26.jar to class loader
[main] INFO  org.apache.spark.util.Utils  - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63369.
[main] INFO  org.apache.spark.network.netty.NettyBlockTransferService  - Server created on 192.168.0.194:63369
[main] INFO  org.apache.spark.storage.BlockManager  - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint  - Registering block manager 192.168.0.194:63369 with 408.9 MiB RAM, BlockManagerId(driver, 192.168.0.194, 63369, None)
[main] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[main] INFO  org.apache.spark.storage.BlockManager  - Initialized BlockManager: BlockManagerId(driver, 192.168.0.194, 63369, None)
[main] INFO  org.apache.spark.sql.internal.SharedState  - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[main] INFO  org.apache.spark.sql.internal.SharedState  - Warehouse path is 'file:/Users/abhishek.somani/src/spark/spark-warehouse'.
[main] INFO  org.apache.spark.sql.hive.HiveUtils  - Initializing HiveMetastoreConnection version 0.13.1 using file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/bonecp-configs.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver-dns_io.netty__netty-resolver-dns__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec_io.netty__netty-codec__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http2_io.netty__netty-codec-http2__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-annotations_com.fasterxml.jackson.core__jackson-annotations__2.11.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-stub_io.grpc__grpc-stub__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.23--org.spark-project.hive.shims__hive-shims-0.23__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-haproxy_io.netty__netty-codec-haproxy__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc-protocol_com.linecorp.armeria__armeria-grpc-protocol__1.0.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--client--client-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-databind_com.fasterxml.jackson.core__jackson-databind__2.11.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_liball_deps_2.12_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-brave_com.linecorp.armeria__armeria-brave__1.0.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/extern--libaws-regions.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.hdrhistogram_HdrHistogram_org.hdrhistogram__HdrHistogram__2.1.12_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.j2objc_j2objc-annotations_com.google.j2objc__j2objc-annotations__1.3_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty-client--jetty-io_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----jackson_annotations_shaded--libjackson-annotations.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter_io.zipkin.reporter2__zipkin-reporter__2.15.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.micrometer_micrometer-core_io.micrometer__micrometer-core__1.5.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--jline--jline--jline__jline__0.9.94.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-tcnative-boringssl-static_io.netty__netty-tcnative-boringssl-static__2.0.31.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.2.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/dbfs--exceptions--exceptions-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.595.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jackson--jsr305_only_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--tracing--tracing-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.zipkin.reporter2_zipkin-reporter-brave_io.zipkin.reporter2__zipkin-reporter-brave__2.15.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_2.4_2.11_deploy_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-common--org.spark-project.hive__hive-common__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common--org.spark-project.hive.shims__hive-shims-common__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_net.bytebuddy_byte-buddy_net.bytebuddy__byte-buddy__1.10.9_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/api-base--api-base-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty-client--jetty-http_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.android_annotations_com.google.android__annotations__4.1.1.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-core_io.grpc__grpc-core__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-ant--org.spark-project.hive__hive-ant__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--log4j--log4j--log4j__log4j__1.2.17.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-context_io.grpc__grpc-context__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/extern--extern-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-dns_io.netty__netty-codec-dns__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--io.netty--netty--io.netty__netty__3.8.0.Final.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-services_io.grpc__grpc-services__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.codehaus.mojo_animal-sniffer-annotations_org.codehaus.mojo__animal-sniffer-annotations__1.18_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--com.microsoft.azure__azure-client-runtime__1.7.8_container_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--antlr--antlr--antlr__antlr__2.7.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_2.4_2.11_deploy_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common_io.netty__netty-transport-native-unix-common__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--path--path-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java_com.google.protobuf__protobuf-java__3.12.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf_io.grpc__grpc-protobuf__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty8-shaded-client--jetty-util_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.10.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.guava_failureaccess_com.google.guava__failureaccess__1.0.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-handler-proxy_io.netty__netty-handler-proxy__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/s3--s3-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.api.grpc_proto-google-common-protos_com.google.api.grpc__proto-google-common-protos__1.17.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.errorprone_error_prone_annotations_com.google.errorprone__error_prone_annotations__2.3.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.latencyutils_LatencyUtils_org.latencyutils__LatencyUtils__2.0.3_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/s3commit--common--common-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.1.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jackson--guava_only_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/api-base--api-base_java-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.checkerframework_checker-compat-qual_org.checkerframework__checker-compat-qual__2.5.5_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--javolution--javolution--javolution__javolution__5.5.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.47.Final.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty-client--jetty-util_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty-client--jetty-client_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-buffer_io.netty__netty-buffer__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jackson--jackson-module-scala-shaded_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.zipkin.zipkin2_zipkin_io.zipkin.zipkin2__zipkin__2.21.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.11_shaded_20180920_b33d810_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.12.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-service--org.spark-project.hive__hive-service__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--jetty--client--client-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http_io.netty__netty-codec-http__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--com.microsoft.rest__client-runtime__1.7.8_container_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--util--locks-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.reactivestreams_reactive-streams_org.reactivestreams__reactive-streams__1.0.3_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-core_com.fasterxml.jackson.core__jackson-core__2.11.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf-lite_io.grpc__grpc-protobuf-lite__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.595.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--oro--oro--oro__oro__2.0.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.595.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.guava_listenablefuture_com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave_io.zipkin.brave__brave__5.12.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.595.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20--org.spark-project.hive.shims__hive-shims-0.20__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.zipkin.brave_brave-instrumentation-http_io.zipkin.brave__brave-instrumentation-http__5.12.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java-util_com.google.protobuf__protobuf-java-util__3.12.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_2.4_2.11_deploy_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--stax--stax-api--stax__stax-api__1.0.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.curioswitch.curiostack_protobuf-jackson_org.curioswitch.curiostack__protobuf-jackson__1.1.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria_com.linecorp.armeria__armeria__1.0.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----jackson_core_shaded--libjackson-core.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.595.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.code.findbugs_jsr305_com.google.code.findbugs__jsr305__3.0.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_jakarta.annotation_jakarta.annotation-api_jakarta.annotation__jakarta.annotation-api__1.3.5_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-handler_io.netty__netty-handler__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.3.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--datalake--datalake-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.protobuf--protobuf-java--org.spark-project.protobuf__protobuf-java__2.5.0-spark.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_org.joda_joda-convert_org.joda__joda-convert__2.2.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.guava_guava_com.google.guava__guava__29.0-android_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.12.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc_com.linecorp.armeria__armeria-grpc__1.0.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty8-shaded-client--jetty-io_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--reflection--reflection-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--credentials--credentials-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.grpc_grpc-api_io.grpc__grpc-api__1.31.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----scalapb_090--com.lihaoyi__sourcecode_2.11__0.1.6_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.27.v20190418.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.json--json--org.json__json__20090211.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-epoll-linux-x86_64_io.netty__netty-transport-native-epoll-linux-x86_64__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----scalapb_090--com.lihaoyi__fastparse_2.11__2.1.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver_io.netty__netty-resolver__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.perfmark_perfmark-api_io.perfmark__perfmark-api__0.19.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20S--org.spark-project.hive.shims__hive-shims-0.20S__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/common--lazy--lazy-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--prometheus-client--simpleclient-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--junit--junit--junit__junit__3.8.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.5.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-serde--org.spark-project.hive__hive-serde__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----jackson_databind_shaded--libjackson-databind.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/daemon--data--client--client-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-common_io.netty__netty-common__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common-secure--org.spark-project.hive.shims__hive-shims-common-secure__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--jackson--paranamer_only_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/s3commit--client--client-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-transport_io.netty__netty-transport__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-socks_io.netty__netty-codec-socks__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_com.google.code.gson_gson_com.google.code.gson__gson__2.8.6_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.11_deploy_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common-linux-x86_64_io.netty__netty-transport-native-unix-common-linux-x86_64__4.1.51.Final_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-shims--org.spark-project.hive__hive-shims__0.13.1a.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--prometheus-client--simpleclient_dropwizard-spark_2.4_2.11_deploy.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.11_shaded_20180625_3682417_spark_2.4.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:file:/Users/abhishek.somani/scratch/metastore-jars-prod-default/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.27.v20190418.jar
[main] INFO  org.apache.spark.sql.hive.client.HiveClientImpl  - Warehouse location for Hive client (version 0.13.1) is file:/Users/abhishek.somani/src/spark/spark-warehouse
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - ObjectStore, initialize called
[main] INFO  DataNucleus.Persistence  - Property spark.hadoop.datanucleus.schema.autoCreateTables unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property datanucleus.schema.autoCreateTables unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[main] INFO  DataNucleus.Persistence  - Property datanucleus.cache.level2 unknown - will be ignored
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Query  - Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
[main] INFO  org.apache.hadoop.hive.metastore.ObjectStore  - Initialized ObjectStore
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - Added admin role in metastore
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - Added public role in metastore
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - No user is added in admin role, since config is empty
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: global_temp
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: global_temp	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 172.307463 ms
[main] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator  - Code generated in 14.635144 ms
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: drop_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=drop_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  DataNucleus.Datastore  - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: create_table: Table(tableName:v, dbName:default, owner:abhishek.somani, createTime:1634577644, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:A, type:int, comment:null), FieldSchema(name:B, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{view.query.out.col.0=A, view.query.out.numCols=2, view.catalogAndNamespace.part.0=spark_catalog, view.catalogAndNamespace.numParts=2, view.query.out.col.1=B, view.referredTempViewNames=[], spark.sql.sources.schema={"type":"struct","fields":[{"name":"A","type":"integer","nullable":false,"metadata":{}},{"name":"B","type":"integer","nullable":false,"metadata":{}}]}, view.catalogAndNamespace.part.1=default, view.referredTempFunctionsNames=[], spark.sql.create.version=3.3.0-SNAPSHOT}, viewOriginalText:select 1 as A, 1 as B, viewExpandedText:select 1 as A, 1 as B, tableType:VIRTUAL_VIEW)
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=create_table: Table(tableName:v, dbName:default, owner:abhishek.somani, createTime:1634577644, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:A, type:int, comment:null), FieldSchema(name:B, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{view.query.out.col.0=A, view.query.out.numCols=2, view.catalogAndNamespace.part.0=spark_catalog, view.catalogAndNamespace.numParts=2, view.query.out.col.1=B, view.referredTempViewNames=[], spark.sql.sources.schema={"type":"struct","fields":[{"name":"A","type":"integer","nullable":false,"metadata":{}},{"name":"B","type":"integer","nullable":false,"metadata":{}}]}, view.catalogAndNamespace.part.1=default, view.referredTempFunctionsNames=[], spark.sql.create.version=3.3.0-SNAPSHOT}, viewOriginalText:select 1 as A, 1 as B, viewExpandedText:select 1 as A, 1 as B, tableType:VIRTUAL_VIEW)	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: alter_table: db=default tbl=v newtbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=alter_table: db=default tbl=v newtbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_database: default
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_database: default	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore  - 0: get_table : db=default tbl=v
[main] INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit  - ugi=abhishek.somani	ip=unknown-ip-addr	cmd=get_table : db=default tbl=v	
[dispatcher-event-loop-2] WARN  org.apache.spark.HeartbeatReceiver  - Removing executor driver with no recent heartbeats: 5661784 ms exceeds timeout 120000 ms
[kill-executor-thread] WARN  org.apache.spark.SparkContext  - Killing executors is not supported by current scheduler.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[executor-heartbeater] INFO  org.apache.spark.executor.Executor  - Told to re-register on heartbeat
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None) re-registering with master
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registering BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster  - Registered BlockManager BlockManagerId(driver, 192.168.0.194, 63369, None)
[executor-heartbeater] INFO  org.apache.spark.storage.BlockManager  - Reporting 0 blocks to the master.
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Invoking stop() from shutdown hook
[shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI  - Stopped Spark web UI at http://192.168.0.194:4040
[dispatcher-event-loop-2] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint  - MapOutputTrackerMasterEndpoint stopped!
[shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore  - MemoryStore cleared
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager  - BlockManager stopped
[shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster  - BlockManagerMaster stopped
[dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint  - OutputCommitCoordinator stopped!
[shutdown-hook-0] INFO  org.apache.spark.SparkContext  - Successfully stopped SparkContext
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Shutdown hook called
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-85beda05-5e00-495f-bb03-9d319ae166ad
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-b7349d3e-46bb-42b4-9747-b86da77daa09/repl-a8157596-670b-4772-9477-c9dddff5fdc3
[shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager  - Deleting directory /private/var/folders/55/w8twbcls4zj46qs0l3mrqhw00000gp/T/spark-b7349d3e-46bb-42b4-9747-b86da77daa09
